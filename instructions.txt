Here is a comprehensive, step-by-step explanation and a professional `README.md` for your project.

### Part 1: What is this project? (In Simple Language)

**The Goal:**
This project is an **AI-powered Tutor** for a **Pepper Robot**. Imagine Pepper as a game show host or a teacher. It shows a user (a child or an adult) a visual puzzle (riddle).
1.  **It Sees:** It displays an image on its tablet.
2.  **It Speaks:** It asks a riddle about the image.
3.  **It Listens:** It waits for the user to answer.
4.  **It Thinks (The Magic):** If the user asks for a "Hint," the robot doesn't just read a pre-written script.
    *   It checks who you are (e.g., "Alex, age 8").
    *   It looks up the puzzle in a PDF document (its "textbook").
    *   It sends all this info to a super-smart AI (like ChatGPT or Claude).
    *   It generates a *unique* hint specifically for an 8-year-old based on that textbook data.

**The Architecture (The "Split Brain"):**
This is the most important technical part.
*   **The Robot (Old Tech):** Pepper runs on an old operating system that only understands **Python 2.7**. It cannot run modern AI libraries.
*   **The Brain (New Tech):** Modern AI (LangChain, OpenAI) requires **Python 3**.
*   **The Solution:** You run two separate programs on your computer. They talk to each other wirelessly using a messaging system called **ZeroMQ**.
    *   *Program A (Python 2.7)* acts as the **Hands and Ears**. It tells the robot to move and listen.
    *   *Program B (Python 3)* acts as the **Brain**. It does the thinking and database work.

---

### Part 2: The `README.md` File

Create a file named `README.md` in the root folder (`pepper_ai_tutor/`) and paste the following content:

```markdown
# Pepper AI Tutor: RAG-Powered Interactive Puzzles

## ðŸ“– Project Overview
This project turns a SoftBank Pepper robot into an intelligent, context-aware tutor. It uses **Retrieval Augmented Generation (RAG)** to allow the robot to "read" a PDF of puzzles and generate personalized hints for users using Large Language Models (LLMs) like Claude 3 or GPT-4.

## ðŸ— Architecture
Because Pepper's NAOqi OS requires Python 2.7, but modern AI libraries require Python 3, this project uses a **Split-Process Architecture** connected via **ZeroMQ**.

1.  **The Brain (Python 3):** Handles logic, database, RAG (Vector Store), and LLM API calls.
2.  **The Body (Python 2.7):** A lightweight listener that translates JSON commands into NAOqi robot actions (Speech, Animation, Tablet).

## âœ¨ Features
*   **User Profiles:** Distinguishes between users (e.g., Kid vs. Adult) and adjusts the AI's personality accordingly.
*   **RAG Memory:** Ingests a PDF (`puzzles.pdf`) to create a searchable knowledge base.
*   **Smart Hints:** Generates dynamic hints using context from the PDF + the User's Persona.
*   **Analytics:** Logs which LLM models are used and how fast they respond.
*   **Model Routing:** Can switch between OpenAI, Anthropic, Google, or DeepSeek via config.

## ðŸ“‚ Folder Structure
```text
pepper_ai_tutor/
â”œâ”€â”€ main_brain_py3/           # THE BRAIN (Modern AI Logic)
â”‚   â”œâ”€â”€ main_controller.py    # Main entry point for the logic
â”‚   â”œâ”€â”€ setup_environment.py  # Run this ONCE to build the database/memory
â”‚   â”œâ”€â”€ config.py             # API Keys and Robot IP
â”‚   â”œâ”€â”€ services/             # Modules for DB, LLM, and RAG
â”‚   â””â”€â”€ data/                 # Source files (users.json, puzzles.pdf)
â”‚
â””â”€â”€ robot_listener_py27/      # THE BODY (Robot Interface)
    â””â”€â”€ robot_listener.py     # Runs in a Python 2.7 env with NAOqi SDK
```

## ðŸš€ Prerequisites

### Hardware
*   SoftBank Pepper Robot
*   A Computer connected to the same Wi-Fi as the robot.

### Software
1.  **Python 3.9+** (For `main_brain_py3`)
2.  **Python 2.7** (For `robot_listener_py27`)
3.  **Aldebaran NAOqi SDK (Python 2.7 version)** installed and added to your PYTHONPATH.

## ðŸ›  Installation & Setup

### 1. Configure the Brain (Python 3)
Navigate to `main_brain_py3/`:
```bash
cd main_brain_py3
pip install -r requirements.txt
```
*   **Edit `config.py`:** Add your real API Keys (OpenAI/Anthropic) and your Robot's IP address.
*   **Edit `data/puzzles.pdf`:** Ensure a valid PDF exists here.
*   **Run Setup:** Initialize the database and AI memory.
```bash
python setup_environment.py
```

### 2. Configure the Body (Python 2.7)
Ensure you have the NAOqi SDK installed. Navigate to `robot_listener_py27/`.
*   **Edit `robot_listener.py`:** Update `ROBOT_IP` to match your robot.

## â–¶ï¸ How to Run

**Step 1: Start the Robot Listener (The Server)**
Open a terminal (using Python 2.7 environment):
```bash
cd robot_listener_py27
python robot_listener.py
```
*You should see: "[Robot Listener] ZeroMQ server started on port 5555..."*

**Step 2: Start the AI Brain (The Client)**
Open a **new** terminal (using Python 3 environment):
```bash
cd main_brain_py3
python main_controller.py
```

## ðŸŽ® Usage Flow
1.  **Login:** The robot asks for your name. Say "Alex" (PIN: 1234) or "pepper" (PIN: 5678).
2.  **Puzzle:** The robot shows an image on the tablet and asks a question.
3.  **Interact:**
    *   Say the answer (e.g., "Egg").
    *   Say "Hint" to get an AI-generated clue based on your profile.
    *   Say "Skip" or "Quit".
```

---

### Part 3: Detailed File Explanation (What does each file do?)

Here is the breakdown of the files provided in your input.

#### A. The Python 3 Brain (`main_brain_py3/`)

1.  **`main_controller.py` (The Conductor)**
    *   **What it does:** This is the main program you run. It controls the flow of the game. It logs the user in, loops through puzzles, decides when to ask the AI for help, and tells the robot what to do.
    *   **Key Logic:** It calls `orchestrator.generate_hint()` when the user says "hint".

2.  **`setup_environment.py` (The Builder)**
    *   **What it does:** You run this **only once** at the beginning.
    *   **Functions:**
        *   Creates the SQLite database (`tutor.db`).
        *   Reads `users.json` and inserts users into the DB.
        *   **Crucial:** It reads `puzzles.pdf`, splits the text, turns it into math (vectors) using OpenAI, and saves it to the `vector_store` folder. This gives the AI its "Memory."

3.  **`config.py` & `settings.py` (The Keys)**
    *   **What it does:** Holds your secrets (API Keys for OpenAI, Anthropic) and configuration (Robot IP address, which model to use).
    *   **Note:** `settings.py` is a safe wrapper that imports from `config.py`.

4.  **`services/robot_proxy.py` (The Messenger)**
    *   **What it does:** The Brain cannot talk to the robot directly. This script sends a JSON message (like a text message) to the `robot_listener.py`.
    *   **Example:** `self.socket.send_json({"action": "say", "text": "Hello"})`.

5.  **`services/database_manager.py` (The Librarian)**
    *   **What it does:** Manages the `tutor.db` file. It saves users, authenticates passwords (PINs), and retrieves puzzle data.

6.  **`services/llm_gateway.py` (The Phone Operator)**
    *   **What it does:** It knows how to talk to different AI companies. If `main_controller` asks for "Claude", this script knows the specific code to call Anthropic's servers. If asked for "GPT", it calls OpenAI.

7.  **`services/langchain_orchestrator.py` (The Strategist)**
    *   **What it does:** This is the smartest part of the system.
    *   **RAG Logic:** It takes the User's Input $\rightarrow$ Searches the Vector Store for relevant info $\rightarrow$ Combines it with the User Profile $\rightarrow$ Creates a perfect prompt for the LLM.

8.  **`services/document_processor.py` (The Reader)**
    *   **What it does:** Uses `PyPDFLoader` to open the PDF and `RecursiveCharacterTextSplitter` to chop it into small paragraphs so the AI can digest it.

9.  **`services/analytics_manager.py` (The Accountant)**
    *   **What it does:** Records how long the AI took to reply and saves it to the database. Useful for seeing if the AI is too slow.

#### B. The Python 2.7 Body (`robot_listener_py27/`)

10. **`robot_listener.py` (The Puppet Master)**
    *   **What it does:** This runs on Python 2.7 (compatible with Pepper). It acts as a **Server**.
    *   **Logic:** It waits for a message from the Brain.
        *   Brain says: `{"action": "play_animation", "data": {"name": "celebrate"}}`
        *   Listener receives it $\rightarrow$ Calls `ALBehaviorManager` (NAOqi) $\rightarrow$ Robot physically dances.

---

### Part 4: What is MISSING? (Crucial for Running)

To actually run this, you need to fix/add these things:

1.  **The PDF File:**
    *   The file `main_brain_py3/data/puzzles.pdf` is missing (you only have the text content in `puzzles.md`). You must create a real PDF with that text inside it and place it in that folder, or the `setup_environment.py` will fail.
2.  **Robot Animations:**
    *   The code calls `play_animation("celebrate")`. You must use the **Choregraphe** software to create an animation, name it "celebrate", and **install it** on the robot's hard drive via the Application Manager.
3.  **NAOqi SDK:**
    *   You need the `pynaoqi` SDK (Python 2.7 version) from the SoftBank Robotics support site. This is hard to find nowadays. Without it, `robot_listener.py` will crash immediately on `import qi`.
4.  **Network Setup:**
    *   Your computer and the Robot **MUST** be on the same Wi-Fi network. The IP address in `config.py` and `robot_listener.py` must match the robot's actual IP.

### Part 5: Step-by-Step Execution Guide

**Step 1: Download & Install**
*   Download all the files into the folder structure shown in Part 2.
*   **Terminal 1 (Python 3):** Install libraries.
    ```bash
    pip install langchain langchain-openai langchain-community pypdf faiss-cpu pyzmq openai anthropic google-generativeai python-dotenv bcrypt
    ```
*   **Terminal 2 (Python 2.7):** Ensure `pyzmq` is installed for Python 2.7 (`pip2 install pyzmq`).

**Step 2: Create the Data**
*   Create a PDF file named `puzzles.pdf` containing the riddles. Put it in `main_brain_py3/data/`.
*   Ensure `users.json` is in `main_brain_py3/data/`.

**Step 3: Setup the Brain**
*   Run: `python3 main_brain_py3/setup_environment.py`
*   *Success:* It should say "Vector store created".

**Step 4: Launch the Listener**
*   Run: `python2 robot_listener_py27/robot_listener.py`
*   *Success:* It will say "ZeroMQ server started...".

**Step 5: Launch the Controller**
*   Run: `python3 main_brain_py3/main_controller.py`
*   *Action:* Speak to your robot!



This is a great situation! You can simulate the entire project using **Choregraphe's Virtual Robot**.

Think of Choregraphe as a "Video Game Version" of Pepper. It runs on your computer. Your Python scripts can talk to this video game version just like they talk to the real robot, with **two major differences**:
1.  **It is deaf:** The virtual robot cannot hear your voice via a microphone.
2.  **It uses a different "Address":** Instead of a Wi-Fi IP address, it lives on `localhost` (your own computer).

Here is the **Step-by-Step Guide** to modifying the project to run without the physical robot.

---

### Phase 1: Prepare Choregraphe

1.  **Open Choregraphe.**
2.  By default, a **Virtual Robot** should appear in the "Robot View" panel.
3.  **Find the Port Number:**
    *   Look at the top menu bar or the "Connection" widget.
    *   Usually, the virtual robot runs on Port **something like `54668`** (it changes every time you restart Choregraphe, or you can set it to fixed in Preferences).
    *   *To be sure:* Go to **Connection > Connect to...**
    *   You will see a robot named "nao" or "pepper" with an IP `127.0.0.1` and a Port (e.g., `51345`). **Write down this Port number.**

---

### Phase 2: Modify `robot_listener.py` (The Body)

Since the virtual robot has no microphone, we need to change the code so that when the Brain asks the robot to "Listen," **you type the answer into the keyboard** instead of speaking it.

Open `robot_listener_py27/robot_listener.py` and make these changes:

#### 1. Change the IP and Port
Find the configuration section at the top and change it to point to your computer:

```python
# --- !! CRITICAL CONFIGURATION !! ---
# Use localhost because Choregraphe is running on THIS computer
ROBOT_IP = "127.0.0.1" 
# CHANGE THIS to the port you saw in Choregraphe (e.g., 54321)
ROBOT_PORT = 12345  # <--- UPDATE THIS NUMBER FROM CHOREGRAPHE
```

#### 2. Fake the "Listen" Function
The virtual robot will crash if we try to use `ALSpeechRecognition`. We will replace the listening logic with a simple `raw_input` (Python 2's way of asking for keyboard input).

**Replace the entire `elif action == "listen":` block with this:**

```python
            elif action == "listen":
                vocabulary = data.get("vocabulary", [])
                timeout = data.get("timeout", 10)
                
                print("\n" + "="*40)
                print("[SIMULATION MODE]")
                print("The Robot is listening for: {}".format(vocabulary))
                print("Please TYPE your answer below (or type 'timeout' to simulate silence):")
                print("="*40 + "\n")

                # specific Python 2 input method
                user_typed = raw_input("YOU SAY: ")

                if user_typed.lower() == "timeout":
                    result = ""
                else:
                    result = user_typed

                return {"status": "ok", "action": "listen", "result": result}
```

#### 3. Update the Tablet Function (Optional)
The virtual robot doesn't have a tablet screen. `ALTabletService` usually works in simulation (it opens a webpage in your browser), but if it crashes, you can wrap it in a try/except block:

```python
            elif action == "show_image":
                try:
                    self.tablet.showImage(str(data.get("url")))
                    print("[SIMULATION] Tablet showing: " + str(data.get("url")))
                except Exception:
                    print("[SIMULATION] Tablet Service not available, but I would show: " + str(data.get("url")))
                return {"status": "ok", "action": "show_image"}
```

---

### Phase 3: Run the Project

Now you are ready to run exactly as if you had a real robot.

#### Step 1: Start the Listener (The Body)
1.  Make sure Choregraphe is open and the Virtual Robot is running.
2.  Open your terminal (Python 2.7 environment).
3.  Run:
    ```bash
    cd robot_listener_py27
    python robot_listener.py
    ```
    *It should say:* `[Robot Listener] NAOqi service proxies are ready.`

#### Step 2: Start the Brain (The Logic)
1.  Open a **second** terminal (Python 3 environment).
2.  Run:
    ```bash
    cd main_brain_py3
    python main_controller.py
    ```

### Phase 4: How to Play (The Simulation)

1.  **Login:**
    *   The Python 3 Brain sends a message.
    *   The Python 2 Listener receives it.
    *   The Virtual Robot in Choregraphe will "Say" (Text-to-speech): *"Hello, please say your username."*
    *   **Action:** Look at your **Python 2 Terminal**. It will ask for input. Type `Alex` and hit Enter.

2.  **The Puzzle:**
    *   The Robot will "Say": *"What has cities but no houses?"*
    *   **Action:** Look at your **Python 2 Terminal**. It will say `[SIMULATION MODE] The Robot is listening...`
    *   Type `Hint` and hit Enter.

3.  **The AI Response:**
    *   The Brain receives "Hint".
    *   It checks the vector store (PDF).
    *   It calls the LLM (OpenAI/Anthropic).
    *   The Robot will "Say" the generated hint.

### Summary of What You Need to Download/Install
Since you are simulating, the list remains mostly the same, but the **Software** requirements are stricter:

1.  **Python 2.7:** Essential for `robot_listener.py`.
2.  **pynaoqi (Python 2.7 SDK):** Even though you have Choregraphe, you still need the `qi` python library installed in your Python 2.7 environment to run the listener script.
    *   *Note:* Choregraphe comes with its own internal Python, but it's hard to run external scripts with it. It is best to have Python 2.7 installed on your Windows/Mac/Linux system and add the path to the Choregraphe SDK to your `PYTHONPATH`.
3.  **Python 3:** For the Brain.
4.  **Libraries:** All the `requirements.txt` libs for Python 3 (`langchain`, `openai`, `zmq`, etc.) and `pyzmq` for Python 2.

**Cheat Sheet for `PYTHONPATH` (If `import qi` fails):**
If you run `python robot_listener.py` and it says `ImportError: No module named qi`, you need to point your computer to Choregraphe's libraries.
*   **Windows:** Set Environment Variable `PYTHONPATH` to `C:\Program Files (x86)\Softbank Robotics\Choregraphe Suite 2.5\lib` (Check your specific path).
*   **Mac:** `export PYTHONPATH=${PYTHONPATH}:/Applications/Choregraphe.app/Contents/Resources/lib/python2.7/site-packages`